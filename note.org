This note provide a guide for different implementations of Shapley Flow.

* exact algorithm is not practical for a dense graph

  Consider a full graph of one timeline with n features.

  For n=20, We would need

  #+BEGIN_SRC python
  def memoize(f):
      d = {}
      def f_(n):
          if n in d: return d[n]
          d[n] = f(n)
          return d[n]
          
      return f_
  
  @memoize
  def T(n):
      if n == 1: return 1
      return sum([T(i) for i in range(1, n)]) + 1

  return T(20)
  #+END_SRC

  #+RESULTS:
  : 524288

  many paths for a single ordering of the graph.This doesn't even count for the
  number of possible permutations of the paths, which in the worst case can be
  $O((n+1)!)$.

  Exact algorithm for this scenario wouldn't be practical. Therefore one need to
  use approximations. Like computing regular Shapley value, one can use
  sampling. However, unlike regular Shapley value, only sampling different
  timelines is not enough because we saw that even a single ordering can be
  prohibitive. We need to sample for the different path.
  
  A hand wavy description of the sampling procedure is the following:

  - sample a source to target path to compute attribution
  - follow edges in the path from the source to the target; for each edge
    - making all input visible, set random set of its sources' non active branches to
      background values and propagate its values downwards (run the eval
      function following a topological ordering)
    - compute the new value for the edge's sink node, assuming all inputs are visible
  - attribute the change in output to the path

  Note that each sampling requires at most $n$ evaluation of the graph because
  the path length is at most $n$. This is a lot cheaper

* efficient divide and conquer solution for balanced tree with small branching factor

  For a balanced tree with n nodes, with a branching factor b

  #+begin_example
  T(n) = (b * b!) T(n/b) + O(b)
       = k (k T(n/b^2) + O(b)) + O(b)
       = O(b) + O(bk) + ... + O(b k^(log_b(n)))
  #+end_example

  where ~k=b*b!~ comes from the fact that at each level, we are computing the
  importance for b nodes, trying all $b!$ orderings. O(b) comes from the fact
  that we are adding the importance of b nodes together to get the importance of
  the top node. One can make this more efficient by ~k=b*2^(b-1)~ where $b-1$
  comes from the fact that we are evaluating the added impact of a feature given
  the different context.
  
  T(n) is clearly dominated by the last term, solving for it yield

  #+begin_example
  T(n) = O((b k)^(log_b(n)))
       = O(2^{(log_2(b) + log_2(k)) * log_2(n) / log_2(b)})
       = O(2^{log_2(n^{1 + log_2(k)/log_2(b)})})
       = O(n^(1 + log_b(k)))
  #+end_example
  
  If b is small, T(n) is a small polynomial in n. When b=2, T(n) = O(n^2), which
  is the time complexity of the partition explainer in ~SHAP~. In general, we
  have $O(n^b)$ for balanced trees.

  This is a vast improvement over the $O(2^n)$ complexity of flat Shapley value.

* Time complexity of implementations

  There are several ways to implement Shapley Flow. Assume we have n nodes, each
  node has $e_i$ number of outgoing edge, and $p$ source to target path. We also
  consider the special case of balanced tree with branching factor $b$. For
  sampling approaches, we denote the number of samplers as k. Denote longest
  path length as l (note $l \leq n$).

** brute force: exact solution

  The brute force way of computing Shapley Flow involves trying all possible
  combinations of outgoing edges for each node, which gives exponential amount
  of orderings to compute. For each ordering, we need to traverse all possible
  paths in the specified order.

  - Time complexity in general: $O(\Pi_{i=1}^n e_i) p$
  - Time complexity for balanced tree: $O(b^n n)$

** divide and conquer: exact solution

   This is the strategy employed for efficient implementation of balanced
   tree. The output at each level is an edge contribution dictionary. This
   method assumes that path ordering for the impact of edges that share the same
   stem does not matter.

   - Time complexity for general case: $O(n!)$
   - Time complexity for balanced tree: $O(n^b)$

** sampling of path: approximation

   This is described earlier. This approach may break the efficiency axiom, but
   is fast.

   - Time complexity for general case: $O(k l)$
   - Time complexity for balanced tree: $O(k log_b(n))$

   This approach can be impelemented on top of the divide and conquer solution
   by sampling an outgoing edge at each node.

** sampling of ordering: approximation

   In this case, we randomly permute edge ordering for each node and perform
   full updates for each sampled ordering. As shown earlier, $p$ could be very
   large. Therefore this approach does not work for dense graph, but it
   maintains the efficiency axiom.

   - Time complexity for general case: $O(k p)$
   - Time complexity for balanced tree: $O(k n)$

   This is the currently implemented approach.

   This approach can be impelemented on top of the divide and conquer solution
   by sampling permutations of outgoing edges at each node.

   - Time complexity for balanced tree: $O(n^(1 + log_b(k)))$
   
** cache

   There are repeated computation to be exploited. Consider the graph (a->b,
   a->c, b->d, b->e), if we already know the ordering of (a->b->e, a->b->d,
   a->c), the ordering (a->b->d, a->b->e, a->c) wouldn't tell us anything new
   about the path contribution of (a->c), therefore we can just add the previous
   contribution (a->c) to update its current value without computing (a->c)
   again. This seems trivial in this case, but if c has many downstream nodes,
   we don't need to compute them all together.

   This should improve the efficiency over all previous approaches.
